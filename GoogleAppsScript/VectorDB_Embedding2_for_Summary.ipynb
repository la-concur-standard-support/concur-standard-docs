{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vgrxd1nh8fsD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bieCkdyFW4CI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnvUqiSEZFkS"
   },
   "source": [
    "# VectorDB_Embedding１：ダイレクトにIndexから文書を選びベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33233,
     "status": "ok",
     "timestamp": 1751528471213,
     "user": {
      "displayName": "la-concur-standard-support",
      "userId": "04856838222038801438"
     },
     "user_tz": -540
    },
    "id": "9Dcrstdii4TM",
    "outputId": "5e82bea7-0d96-4955-d8c2-5572fce2ec7a"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y openai\n",
    "!pip install openai==0.28.0\n",
    "\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv\n",
    "!pip install pinecone==6.0.1\n",
    "\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 48512,
     "status": "ok",
     "timestamp": 1751528737619,
     "user": {
      "displayName": "la-concur-standard-support",
      "userId": "04856838222038801438"
     },
     "user_tz": -540
    },
    "id": "Og1EzEUGp4I1",
    "outputId": "495efa23-becb-44b7-8632-242a4cb91287"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from pinecone import NotFoundException\n",
    "\n",
    "# Colab用: ファイルアップロードGUI\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    files = None\n",
    "\n",
    "############################\n",
    "# 1) APIキー読み込み\n",
    "############################\n",
    "def load_api_keys(txt_filepath=\"api_keys_standard.txt\"):\n",
    "    if not os.path.exists(txt_filepath):\n",
    "        raise FileNotFoundError(f\"APIキーのファイルが見つかりません: {txt_filepath}\")\n",
    "    with open(txt_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" in line:\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            os.environ[k.strip()] = v.strip()\n",
    "\n",
    "load_api_keys(\"api_keys_standard.txt\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "OPENAI_API_KEY   = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "openai.api_key   = OPENAI_API_KEY\n",
    "\n",
    "INDEX_NAME = \"concur-index-summary\"\n",
    "NAMESPACE  = \"demo-html\"\n",
    "\n",
    "############################\n",
    "# 2) Pinecone 初期化\n",
    "############################\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "try:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\"\n",
    "    )\n",
    "    print(f\"[INFO] Created new Pinecone index: {INDEX_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Index creation skipped or already exists: {e}\")\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "stats = index.describe_index_stats()\n",
    "ns_info = stats.get(\"namespaces\", {})\n",
    "if NAMESPACE in ns_info:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' 既に存在: vector_count={ns_info[NAMESPACE]['vector_count']}\")\n",
    "else:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' はまだ存在しません。\")\n",
    "\n",
    "############################\n",
    "# 3) indexページをパース (HTMLスクレイプ)\n",
    "############################\n",
    "def parse_index_page(index_url):\n",
    "    resp = requests.get(index_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    result_list = []\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        print(\"[WARN] テーブルが見つかりません:\", index_url)\n",
    "        return result_list\n",
    "\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        guide_en = cols[0].get_text(strip=True)\n",
    "        guide_jp = cols[1].get_text(strip=True)\n",
    "        base_url = cols[3].get_text(strip=True)\n",
    "        result_list.append({\n",
    "            \"GuideNameEn\": guide_en,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"BaseURL\": base_url\n",
    "        })\n",
    "    return result_list\n",
    "\n",
    "############################\n",
    "# Embedding関連\n",
    "############################\n",
    "def get_embedding(text: str, model=\"text-embedding-ada-002\"):\n",
    "    resp = openai.Embedding.create(model=model, input=text)\n",
    "    return resp[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def chunk_text(text: str, chunk_size=2000):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "    while start < length:\n",
    "        end = min(start + chunk_size, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "############################\n",
    "# DOCXテキスト抽出\n",
    "############################\n",
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(filepath):\n",
    "    doc = Document(filepath)\n",
    "    paragraphs = [p.text for p in doc.paragraphs]\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "############################\n",
    "# メインフロー\n",
    "############################\n",
    "def main():\n",
    "    # 1) indexページからガイド一覧取得\n",
    "    index_url = \"https://la-concur-standard-support.github.io/concur-standard-docs/index.htm\"\n",
    "    docs = parse_index_page(index_url)\n",
    "    if not docs:\n",
    "        print(\"[ERROR] indexページからガイド情報が取得できませんでした。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== ガイド一覧 ===\")\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        print(f\"{i}. {d['GuideNameEn']} / {d['GuideNameJp']} => {d['BaseURL']}\")\n",
    "\n",
    "    sel = input(\"\\nどのガイドを処理しますか？(番号): \").strip()\n",
    "    if not sel.isdigit():\n",
    "        print(\"キャンセル\")\n",
    "        return\n",
    "    idx = int(sel)\n",
    "    if idx < 1 or idx > len(docs):\n",
    "        print(\"[ERROR] 選択が範囲外。終了。\")\n",
    "        return\n",
    "\n",
    "    chosen = docs[idx-1]\n",
    "    base_url = chosen[\"BaseURL\"]\n",
    "    guide_en = chosen[\"GuideNameEn\"]\n",
    "    guide_jp = chosen[\"GuideNameJp\"]\n",
    "\n",
    "    fname = os.path.basename(urlsplit(base_url).path)\n",
    "    doc_name = re.sub(r\"\\.html?$\", \".docx\", fname, flags=re.IGNORECASE)\n",
    "\n",
    "    print(f\"\\n選択されたガイド => doc_name={doc_name}, base_url={base_url}\")\n",
    "    print(\"[INFO] これからPCのフォルダにある要約ファイルをアップロードしてください。\")\n",
    "\n",
    "    summary_text = \"\"\n",
    "    if files is not None:\n",
    "        uploaded = files.upload()\n",
    "        if not uploaded:\n",
    "            print(\"ファイルがアップロードされませんでした。終了します。\")\n",
    "            return\n",
    "\n",
    "        # 先頭に1ファイルのみ対応\n",
    "        for fn in uploaded.keys():\n",
    "            ext = os.path.splitext(fn)[1].lower()\n",
    "            if ext == \".docx\":\n",
    "                # docx を python-docx で解析\n",
    "                summary_text = extract_text_from_docx(fn)\n",
    "            else:\n",
    "                # 通常のテキストファイルとして扱う\n",
    "                with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
    "                    summary_text = f.read()\n",
    "\n",
    "            print(f\"[INFO] ファイル '{fn}' を読み込みました。\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        # Colab以外の場合\n",
    "        local_path = input(\"要約ファイルのパスを入力してください: \").strip()\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"[ERROR] ファイルが見つかりません: {local_path}\")\n",
    "            return\n",
    "\n",
    "        ext = os.path.splitext(local_path)[1].lower()\n",
    "        if ext == \".docx\":\n",
    "            summary_text = extract_text_from_docx(local_path)\n",
    "        else:\n",
    "            with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                summary_text = f.read()\n",
    "\n",
    "        print(f\"[INFO] ファイル '{local_path}' を読み込みました。\")\n",
    "\n",
    "    if not summary_text:\n",
    "        print(\"[ERROR] テキストが空です。終了します。\")\n",
    "        return\n",
    "\n",
    "    # chunk 分割\n",
    "    chunk_list = chunk_text(summary_text, chunk_size=2000)\n",
    "\n",
    "    ############################\n",
    "    # Pinecone バッチアップサート\n",
    "    ############################\n",
    "    BATCH_SIZE = 100\n",
    "    vectors_buffer = []\n",
    "    doc_id_counter = 0\n",
    "\n",
    "    def flush_upsert_buffer():\n",
    "        nonlocal vectors_buffer\n",
    "        if not vectors_buffer:\n",
    "            return\n",
    "        print(f\"[BATCH UPSERT] {len(vectors_buffer)} vectors ...\")\n",
    "        resp = index.upsert(vectors=vectors_buffer, namespace=NAMESPACE)\n",
    "        print(\"[BATCH RESP]:\", resp)\n",
    "        vectors_buffer = []\n",
    "        time.sleep(1)\n",
    "\n",
    "    for chunk_str_data in chunk_list:\n",
    "        doc_id_counter += 1\n",
    "        chunk_id = f\"{doc_name}_chunk{doc_id_counter}\"\n",
    "\n",
    "        emb = get_embedding(chunk_str_data)\n",
    "        meta = {\n",
    "            \"DocName\": doc_name,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"SectionTitle1\": \"\",\n",
    "            \"SectionTitle2\": \"\",\n",
    "            \"AnchorID\": \"\",\n",
    "            \"FullLink\": base_url,\n",
    "            \"chunk_text\": chunk_str_data\n",
    "        }\n",
    "        vectors_buffer.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"values\": emb,\n",
    "            \"metadata\": meta\n",
    "        })\n",
    "\n",
    "        if len(vectors_buffer) >= BATCH_SIZE:\n",
    "            flush_upsert_buffer()\n",
    "\n",
    "    flush_upsert_buffer()\n",
    "    print(\"[INFO] 全アップサート完了\")\n",
    "\n",
    "    # テスト検索\n",
    "    test_query = \"ConcurのAPIでレシート登録したい\"\n",
    "    print(f\"\\n[INFO] テスト検索: {test_query}\")\n",
    "    q_emb = get_embedding(test_query)\n",
    "    sr = index.query(\n",
    "        vector=q_emb,\n",
    "        top_k=3,\n",
    "        include_metadata=True,\n",
    "        namespace=NAMESPACE\n",
    "    )\n",
    "\n",
    "    for match in sr.matches:\n",
    "        if not match.metadata:\n",
    "            print(f\"- ID={match.id}, Score={match.score}\")\n",
    "            print(\"  (No metadata found)\\n\")\n",
    "            continue\n",
    "\n",
    "        md = match.metadata\n",
    "        print(f\"- ID={match.id}, Score={match.score}\")\n",
    "        print(\"  SectionTitle2:\", md.get(\"SectionTitle2\"))\n",
    "        print(\"  FullLink:\", md.get(\"FullLink\"))\n",
    "        print(\"  chunk_text:\", md.get(\"chunk_text\",\"\")[:80], \"...\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvCSzhxIY_by"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx79oU3pNQtz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9xAcfF8Y8b6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMuRVvjPZbPq"
   },
   "source": [
    "## VectorDB_Embedding2：手元のWORD文書からベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boXKeWJ8Y9Co"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y openai\n",
    "!pip install openai==0.28.0\n",
    "\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv\n",
    "!pip install pinecone==6.0.1\n",
    "!pip install python-docx\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import docx\n",
    "import pandas as pd\n",
    "import time  # ← バッチアップサート時に少し待機するため\n",
    "from urllib.parse import urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "############################\n",
    "# 1) APIキー読み込み\n",
    "############################\n",
    "def load_api_keys(txt_filepath=\"api_keys_standard.txt\"):\n",
    "    if not os.path.exists(txt_filepath):\n",
    "        raise FileNotFoundError(f\"APIキーのファイルが見つかりません: {txt_filepath}\")\n",
    "\n",
    "    with open(txt_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" in line:\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            os.environ[k.strip()] = v.strip()\n",
    "\n",
    "load_api_keys(\"api_keys_standard.txt\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "OPENAI_API_KEY   = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "openai.api_key   = OPENAI_API_KEY\n",
    "\n",
    "INDEX_NAME = \"concur-index-summary\"\n",
    "NAMESPACE  = \"demo-html\"\n",
    "\n",
    "############################\n",
    "# 2) Pinecone 初期化\n",
    "############################\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "stats = index.describe_index_stats()\n",
    "ns_info = stats.get(\"namespaces\", {})\n",
    "if NAMESPACE in ns_info:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' 既に存在: vector_count={ns_info[NAMESPACE]['vector_count']}\")\n",
    "else:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' はまだ存在しません。\")\n",
    "\n",
    "\n",
    "############################\n",
    "# 3) indexページ (HTML) をパース\n",
    "############################\n",
    "def parse_index_page(index_url):\n",
    "    resp = requests.get(index_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    result_list = []\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        print(\"[WARN] テーブルが見つかりません:\", index_url)\n",
    "        return result_list\n",
    "\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        guide_en = cols[0].get_text(strip=True)\n",
    "        guide_jp = cols[1].get_text(strip=True)\n",
    "        base_url = cols[3].get_text(strip=True)\n",
    "        result_list.append({\n",
    "            \"GuideNameEn\": guide_en,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"BaseURL\": base_url\n",
    "        })\n",
    "    return result_list\n",
    "\n",
    "############################\n",
    "# 4) mapping_DB.csv ロード\n",
    "############################\n",
    "def load_mapping_db(csv_path=\"mapping_DB.csv\"):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"[ERROR] mapping_DB.csv が見つかりません: {csv_path}\")\n",
    "\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "############################\n",
    "# 5) HTMLソース取得\n",
    "############################\n",
    "def fetch_html(url):\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "############################\n",
    "# 6) アンカーoffset検索\n",
    "############################\n",
    "def find_anchor_offset(html_src, anchor_id):\n",
    "    pattern = re.compile(\n",
    "        rf'<a[^>]+(?:id|name)\\s*=\\s*[\"\\']{re.escape(anchor_id)}[\"\\']',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m = pattern.search(html_src)\n",
    "    if m:\n",
    "        return m.start()\n",
    "    return -1\n",
    "\n",
    "############################\n",
    "# 7) chunking & embedding\n",
    "############################\n",
    "def get_embedding(text: str, model=\"text-embedding-ada-002\"):\n",
    "    resp = openai.Embedding.create(model=model, input=text)\n",
    "    return resp[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def chunk_text(text: str, chunk_size=2000):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "    while start < length:\n",
    "        end = min(start + chunk_size, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "############################\n",
    "# 8) Word文書(ローカル) からテキスト抽出\n",
    "############################\n",
    "def parse_local_word(docx_path):\n",
    "    if not os.path.exists(docx_path):\n",
    "        raise FileNotFoundError(f\"[ERROR] Word文書が見つかりません: {docx_path}\")\n",
    "\n",
    "    doc = docx.Document(docx_path)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    full_text = \"\\n\".join(paragraphs)\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) indexページからdocリストを取得\n",
    "    index_url = \"https://la-concur-standard-support.github.io/concur-standard-docs/index.htm\"\n",
    "    docs = parse_index_page(index_url)\n",
    "    if not docs:\n",
    "        print(\"[ERROR] indexページが見つかりません。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== ガイド一覧 ===\")\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        print(f\"{i}. {d['GuideNameEn']} / {d['GuideNameJp']} => {d['BaseURL']}\")\n",
    "\n",
    "    sel = input(\"\\nどのガイドを処理しますか？(番号): \").strip()\n",
    "    if not sel.isdigit():\n",
    "        print(\"[ERROR] キャンセル or 無効入力\")\n",
    "        return\n",
    "    idx = int(sel)\n",
    "    if idx < 1 or idx > len(docs):\n",
    "        print(\"[ERROR] 範囲外\")\n",
    "        return\n",
    "\n",
    "    chosen = docs[idx-1]\n",
    "    base_url = chosen[\"BaseURL\"]\n",
    "    guide_en = chosen[\"GuideNameEn\"]\n",
    "    guide_jp = chosen[\"GuideNameJp\"]\n",
    "\n",
    "    # 例: \"Exp_SG_Account_Codes-jp.html\" => \"Exp_SG_Account_Codes-jp.docx\"\n",
    "    fname = os.path.basename(urlsplit(base_url).path)\n",
    "    doc_name = re.sub(r\"\\.html?$\", \".docx\", fname, flags=re.IGNORECASE)\n",
    "\n",
    "    print(f\"\\n選択ガイド => doc_name={doc_name}, base_url={base_url}\")\n",
    "\n",
    "    # 2) mapping_DB.csv ロード\n",
    "    mapping_rows = load_mapping_db(\"mapping_DB.csv\")\n",
    "    if not mapping_rows:\n",
    "        print(\"[ERROR] mapping_DB.csv 読み込み失敗\")\n",
    "        return\n",
    "\n",
    "    # doc_name に一致する行\n",
    "    doc_rows = [r for r in mapping_rows if r.get(\"DocName\", \"\") == doc_name]\n",
    "\n",
    "    ############################\n",
    "    # ▼▼ バッチアップサート ▼▼\n",
    "    ############################\n",
    "\n",
    "    # Pineconeアップサート用の設定\n",
    "    BATCH_SIZE = 100\n",
    "    vectors_buffer = []\n",
    "    doc_id_counter = 0\n",
    "\n",
    "    def flush_upsert_buffer():\n",
    "        \"\"\"バッファの内容をまとめてUpsertし、バッファをクリア\"\"\"\n",
    "        nonlocal vectors_buffer\n",
    "        if not vectors_buffer:\n",
    "            return\n",
    "        print(f\"[BATCH UPSERT] {len(vectors_buffer)} vectors ...\")\n",
    "        resp = index.upsert(vectors=vectors_buffer, namespace=NAMESPACE)\n",
    "        print(\"[BATCH RESP]:\", resp)\n",
    "        # バッファをクリア\n",
    "        vectors_buffer = []\n",
    "        # 連打を避けるため少し待機\n",
    "        time.sleep(1)\n",
    "\n",
    "    # ================================\n",
    "    # (A) HTMLダウンロードの試行\n",
    "    # ================================\n",
    "    can_download = True\n",
    "    html_src = \"\"\n",
    "    try:\n",
    "        print(f\"[INFO] ダウンロード試行: {base_url}\")\n",
    "        html_src = fetch_html(base_url)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] ダウンロード失敗: {e}\")\n",
    "        can_download = False\n",
    "\n",
    "    if can_download and doc_rows:\n",
    "        # --- HTMLが成功ダウンロード → mapping_DBに従い、アンカー単位で分割 ---\n",
    "        print(\"[INFO] Webダウンロード成功 → mapping_DBに従い、アンカー単位で分割します。\")\n",
    "\n",
    "        # PageNumber + AnchorID でソート\n",
    "        def sort_key(r):\n",
    "            p = 999999\n",
    "            if r[\"PageNumber\"].isdigit():\n",
    "                p = int(r[\"PageNumber\"])\n",
    "            return (p, r[\"AnchorID\"])\n",
    "        sorted_rows = sorted(doc_rows, key=sort_key)\n",
    "\n",
    "        offsets = []\n",
    "        for row in sorted_rows:\n",
    "            anchor_id = row[\"AnchorID\"]\n",
    "            off = find_anchor_offset(html_src, anchor_id)\n",
    "            offsets.append({\"row\": row, \"offset\": off})\n",
    "\n",
    "        for i in range(len(offsets)):\n",
    "            cur = offsets[i]\n",
    "            r   = cur[\"row\"]\n",
    "            off_i = cur[\"offset\"]\n",
    "            if off_i < 0:\n",
    "                off_i = 0\n",
    "\n",
    "            if i < len(offsets)-1:\n",
    "                off_j = offsets[i+1][\"offset\"]\n",
    "                if off_j < 0:\n",
    "                    off_j = len(html_src)\n",
    "            else:\n",
    "                off_j = len(html_src)\n",
    "            if off_j < off_i:\n",
    "                off_j = off_i\n",
    "\n",
    "            raw_segment = html_src[off_i:off_j]\n",
    "            text_str = re.sub(r\"<[^>]+>\", \"\", raw_segment)\n",
    "            text_str = re.sub(r\"\\s+\", \" \", text_str).strip()\n",
    "            if not text_str:\n",
    "                continue\n",
    "\n",
    "            chunk_list = chunk_text(text_str, chunk_size=2000)\n",
    "\n",
    "            doc_name_val = r[\"DocName\"]\n",
    "            guide_jp_val = r[\"GuideNameJp\"]\n",
    "            sec1         = r[\"SectionTitle1\"]\n",
    "            sec2         = r[\"SectionTitle2\"]\n",
    "            anchor_val   = r[\"AnchorID\"]\n",
    "            link_val     = r[\"FullLink\"]\n",
    "\n",
    "            for chunk_str_data in chunk_list:\n",
    "                doc_id_counter += 1\n",
    "                chunk_id = f\"{doc_name_val}_chunk{doc_id_counter}\"\n",
    "\n",
    "                emb = get_embedding(chunk_str_data)\n",
    "                meta = {\n",
    "                    \"DocName\": doc_name_val,\n",
    "                    \"GuideNameJp\": guide_jp_val,\n",
    "                    \"SectionTitle1\": sec1,\n",
    "                    \"SectionTitle2\": sec2,\n",
    "                    \"AnchorID\": anchor_val,\n",
    "                    \"FullLink\": link_val,\n",
    "                    \"chunk_text\": chunk_str_data\n",
    "                }\n",
    "\n",
    "                vectors_buffer.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"values\": emb,\n",
    "                    \"metadata\": meta\n",
    "                })\n",
    "\n",
    "                # バッファが一定サイズに達したらUpsert\n",
    "                if len(vectors_buffer) >= BATCH_SIZE:\n",
    "                    flush_upsert_buffer()\n",
    "\n",
    "        # ループ完了後、残りがあればフラッシュ\n",
    "        flush_upsert_buffer()\n",
    "        print(\"[INFO] HTML文書のアップサート完了.\")\n",
    "\n",
    "    else:\n",
    "        # ================================\n",
    "        # (B) Word文書がローカルにあるパターン\n",
    "        # ================================\n",
    "        print(\"\\n[INFO] Word文書のローカルアップロード対応に切り替えます。\")\n",
    "        local_path = input(\"Word文書 (.docx) のローカルファイルパスを入力してください: \").strip()\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"[ERROR] 指定ファイルが見つかりません: {local_path}\")\n",
    "            return\n",
    "\n",
    "        # parse_local_word でテキスト全抽出\n",
    "        full_text = parse_local_word(local_path)\n",
    "        print(f\"[INFO] Word文書の段落合計文字数={len(full_text)}\")\n",
    "\n",
    "        # chunk化 & Embedding\n",
    "        chunked_data = chunk_text(full_text, chunk_size=2000)\n",
    "\n",
    "        doc_name_val = doc_name\n",
    "        guide_jp_val = guide_jp\n",
    "\n",
    "        for chunk_str_data in chunked_data:\n",
    "            doc_id_counter += 1\n",
    "            chunk_id = f\"{doc_name_val}_chunk{doc_id_counter}\"\n",
    "\n",
    "            emb = get_embedding(chunk_str_data)\n",
    "            meta = {\n",
    "                \"DocName\": doc_name_val,\n",
    "                \"GuideNameJp\": guide_jp_val,\n",
    "                \"FullLink\": \"(Local file, no link)\",\n",
    "                \"chunk_text\": chunk_str_data\n",
    "            }\n",
    "            vectors_buffer.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"values\": emb,\n",
    "                \"metadata\": meta\n",
    "            })\n",
    "\n",
    "            # バッファが一定サイズに達したらUpsert\n",
    "            if len(vectors_buffer) >= BATCH_SIZE:\n",
    "                flush_upsert_buffer()\n",
    "\n",
    "        # ループ完了後、残りがあればフラッシュ\n",
    "        flush_upsert_buffer()\n",
    "        print(\"[INFO] ローカルWord文書のアップサート完了.\")\n",
    "\n",
    "    # テスト検索\n",
    "    test_query = \"ConcurのAPIでレシート登録したい\"\n",
    "    print(f\"\\n[INFO] テスト検索: {test_query}\")\n",
    "    q_emb = get_embedding(test_query)\n",
    "    sr = index.query(\n",
    "        vector=q_emb,\n",
    "        top_k=3,\n",
    "        include_metadata=True,\n",
    "        namespace=NAMESPACE\n",
    "    )\n",
    "    for match in sr.matches:\n",
    "        md = match.metadata\n",
    "        print(f\"- ID={match.id}, Score={match.score}\")\n",
    "        print(\"  DocName:\", md.get(\"DocName\"))\n",
    "        print(\"  chunk_text:\", md.get(\"chunk_text\",\"\")[:80], \"...\")\n",
    "        print(\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2quo1S5LT6K"
   },
   "source": [
    "### Pineconeの全データを消去する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "la7Ijvo9J8TE",
    "outputId": "bf3b24b7-c7b4-4ec0-abd7-fbd3ad65e10b"
   },
   "outputs": [],
   "source": [
    "# Pinecone クライアントをインストール\n",
    "!pip install pinecone-client\n",
    "\n",
    "# 必要なライブラリをインポート\n",
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone\n",
    "\n",
    "############################\n",
    "# 1) APIキーの読み込み（Colab 用）\n",
    "############################\n",
    "API_KEY_FILE = \"api_keys_standard.txt\"\n",
    "\n",
    "def load_api_keys(filepath=API_KEY_FILE):\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"[ERROR] APIキーのファイルが見つかりません: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" in line:\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            os.environ[key.strip()] = value.strip()\n",
    "\n",
    "# APIキーをロード\n",
    "load_api_keys(API_KEY_FILE)\n",
    "\n",
    "# Pinecone API キー取得\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "INDEX_NAME = \"concur-index-summary\"  # ここを実際のインデックス名に変更\n",
    "NAMESPACE = \"demo-html\"  # ここを適切な値に変更\n",
    "\n",
    "############################\n",
    "# 2) Pinecone 初期化 & 全データ削除\n",
    "############################\n",
    "try:\n",
    "    # Pinecone クライアント初期化\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    # インデックスに接続\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "\n",
    "    # インデックスの情報を取得\n",
    "    stats = index.describe_index_stats()\n",
    "    namespaces = stats.get(\"namespaces\", {}).keys()  # すべての名前空間を取得\n",
    "\n",
    "    # すべてのデータを削除\n",
    "    for namespace in namespaces:\n",
    "        print(f\"[INFO] Namespace '{namespace}' のデータを削除中...\")\n",
    "        index.delete(delete_all=True, namespace=namespace)\n",
    "        time.sleep(1)  # Pineconeの負荷を考慮して1秒待機\n",
    "\n",
    "    print(\"[SUCCESS] すべてのデータを削除しました！\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Pinecone のデータ削除中にエラーが発生: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLtNoLUxJ-BF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
