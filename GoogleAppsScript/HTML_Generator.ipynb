{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyH5THNNFaOL"
   },
   "source": [
    "# HTML Generaror 1: 全自動でWORD版の開発ガイドをHTML版に移植"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31995,
     "status": "ok",
     "timestamp": 1751525191418,
     "user": {
      "displayName": "la-concur-standard-support",
      "userId": "04856838222038801438"
     },
     "user_tz": -540
    },
    "id": "PVZM9hn1uPvU",
    "outputId": "d2d76c0e-291f-49fe-87fa-54be9d09d058"
   },
   "outputs": [],
   "source": [
    "# Colabセルにまとめて貼り付けて実行\n",
    "# 1) LibreOffice/Mammoth等のインストール\n",
    "# 2) indexページ解析 (docDownloadURL)\n",
    "# 3) doc->docx\n",
    "# 4) docx->html\n",
    "# 5) HTML上部の枠を削除\n",
    "\n",
    "################################\n",
    "# 事前インストール:\n",
    "################################\n",
    "!apt-get update -qq\n",
    "!apt-get install -y libreoffice\n",
    "!pip install mammoth requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20403,
     "status": "ok",
     "timestamp": 1751525436465,
     "user": {
      "displayName": "la-concur-standard-support",
      "userId": "04856838222038801438"
     },
     "user_tz": -540
    },
    "id": "Y8gtmdtmuoSX",
    "outputId": "4fe7256f-94f1-48d0-f29e-a5214962613d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Colabセルにまとめて貼り付けて実行 (Word→HTML変換＋後処理)\n",
    "# 1) LibreOffice/Mammoth等のインストール\n",
    "# 2) indexページ解析 (docDownloadURL)\n",
    "# 3) doc->docx\n",
    "# 4) docx->html\n",
    "# 5) remove_top_frame (画像/枠削除)\n",
    "# 6) remove_table_border_none (必要に応じて)\n",
    "# 7) apply_table_borders (強制CSS追加)\n",
    "# 8) remove_top_until_keyword_keep_previous (「設定ガイド」より前の要素を削除)\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import mammoth\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "\n",
    "def parse_index_page(index_url):\n",
    "    \"\"\"Indexページの<table>を解析し (GuideNameEn, GuideNameJp, DocDownloadURL, BaseURL) を返す\"\"\"\n",
    "    resp = requests.get(index_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    result_list = []\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        print(\"テーブルが見つかりません。HTML構造を確認してください。\")\n",
    "        return result_list\n",
    "\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        guide_en = cols[0].get_text(strip=True)\n",
    "        guide_jp = cols[1].get_text(strip=True)\n",
    "        download_td = cols[2]\n",
    "        base_url = cols[3].get_text(strip=True)\n",
    "\n",
    "        # DOCリンク探索\n",
    "        doc_download_url = None\n",
    "        for a in download_td.find_all(\"a\"):\n",
    "            href = a.get(\"href\",\"\")\n",
    "            if href.lower().endswith(\".doc\") or href.lower().endswith(\".docx\"):\n",
    "                doc_download_url = urljoin(index_url, href)\n",
    "                break\n",
    "\n",
    "        result_list.append({\n",
    "            \"GuideNameEn\": guide_en,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"DocDownloadURL\": doc_download_url,\n",
    "            \"BaseURL\": base_url\n",
    "        })\n",
    "    return result_list\n",
    "\n",
    "def doc_to_docx_via_libreoffice(doc_path):\n",
    "    \"\"\"soffice --convert-to docx を使って .doc → .docx\"\"\"\n",
    "    soffice_path = \"/usr/bin/soffice\"\n",
    "    cmd = [\n",
    "        soffice_path, \"--headless\", \"--convert-to\", \"docx\",\n",
    "        doc_path, \"--outdir\",\n",
    "        os.path.dirname(doc_path) if os.path.dirname(doc_path) else \".\"\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "    base, ext = os.path.splitext(doc_path)\n",
    "    docx_file = base + \".docx\"\n",
    "    if not os.path.exists(docx_file):\n",
    "        raise FileNotFoundError(f\"{docx_file} が生成されませんでした。.doc→.docx失敗？\")\n",
    "    return docx_file\n",
    "\n",
    "def mammoth_docx_to_html(docx_file, html_file):\n",
    "    \"\"\"Mammoth で docx→HTML\"\"\"\n",
    "    with open(docx_file, \"rb\") as f:\n",
    "        result = mammoth.convert_to_html(f)\n",
    "    html_content = result.value\n",
    "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_content)\n",
    "    print(f\"[OK] {docx_file} → {html_file} (HTML化)\")\n",
    "\n",
    "def ensure_body(soup):\n",
    "    \"\"\"body が無い場合は作って移動\"\"\"\n",
    "    if not soup.body:\n",
    "        new_body = soup.new_tag(\"body\")\n",
    "        # head要素以外の子要素を body に移動\n",
    "        for child in list(soup.contents):\n",
    "            if child.name != \"head\":\n",
    "                new_body.append(child.extract())\n",
    "        soup.append(new_body)\n",
    "    return soup\n",
    "\n",
    "def remove_top_frame(html_path):\n",
    "    \"\"\"表紙の枠(div style='border...')と先頭の大きな画像(p>img base64)を削除\"\"\"\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    soup = ensure_body(soup)\n",
    "    body = soup.body\n",
    "\n",
    "    # (1) <div style=\"border:...\"> 削除\n",
    "    candidate_divs = body.find_all(\"div\", style=re.compile(r\"(?i)border\"))\n",
    "    if candidate_divs:\n",
    "        top_div = candidate_divs[0]\n",
    "        top_div.decompose()\n",
    "\n",
    "    # (2) 先頭に出る base64画像っぽい <p><img...> を削除\n",
    "    paragraphs = body.find_all(\"p\")\n",
    "    for ptag in paragraphs:\n",
    "        img = ptag.find(\"img\", src=re.compile(r\"^data:image/png;base64\"))\n",
    "        if img:\n",
    "            ptag.decompose()\n",
    "            break\n",
    "\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "    print(f\"[OK] 表紙枠/画像を削除 => {html_path}\")\n",
    "\n",
    "def force_inline_table_border(html_path):\n",
    "    \"\"\"\n",
    "    (A) tableタグに border='1' を付与\n",
    "    (B) table/tr/td/th の inline-style から 'border:none' を除去\n",
    "    (C) 最後に border:1px solid black を追加\n",
    "    \"\"\"\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    soup = ensure_body(soup)\n",
    "    body = soup.body\n",
    "\n",
    "    # (A) tableタグに border='1' を強制\n",
    "    tables = body.find_all(\"table\")\n",
    "    for tbl in tables:\n",
    "        tbl['border'] = \"1\"  # 古いHTML方式\n",
    "        # もしセル間スペースをなくしたいなら:\n",
    "        # tbl['cellspacing'] = \"0\"\n",
    "        # tbl['cellpadding'] = \"4\"\n",
    "\n",
    "    # (B)(C) table, tr, th, td の inline-styleを上書き\n",
    "    for tag in body.find_all([\"table\",\"tr\",\"th\",\"td\"]):\n",
    "        old_style = tag.get(\"style\",\"\")\n",
    "        # 'border: none' を削除\n",
    "        new_style = re.sub(r\"border\\s*:\\s*none[^;]*;?\", \"\", old_style, flags=re.IGNORECASE)\n",
    "        # border:1px solid blackを追記(既にあっても重複OK)\n",
    "        # デモのため \"border-collapse\" まで追加するなら table だけにやるとか\n",
    "        if tag.name == \"table\":\n",
    "            # tableには border-collapse: collapse;\n",
    "            # (inline-styleに !important は書けないのでなるべく conflicts のないルールを…)\n",
    "            new_style += \";border:1px solid black;border-collapse:collapse;\"\n",
    "        else:\n",
    "            new_style += \";border:1px solid black;\"\n",
    "        tag[\"style\"] = new_style.strip(\"; \")\n",
    "\n",
    "    # 書き戻し\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "    print(f\"[OK] テーブル枠線をインラインで強制 => {html_path}\")\n",
    "\n",
    "def remove_top_until_keyword_keep_previous(html_file, keyword=\"設定ガイド\"):\n",
    "    \"\"\"キーワードを含む<p>の直前の<p>だけ残し、それより前の要素は削除\"\"\"\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    soup = ensure_body(soup)\n",
    "    body = soup.body\n",
    "\n",
    "    target_p = body.find(\"p\", string=lambda txt: txt and keyword in txt)\n",
    "    if not target_p:\n",
    "        print(f\"'{keyword}' を含む <p> が見つからないためスキップ\")\n",
    "        with open(html_file, \"w\", encoding=\"utf-8\") as f2:\n",
    "            f2.write(str(soup))\n",
    "        return\n",
    "\n",
    "    prev_p = None\n",
    "    for sib in target_p.previous_siblings:\n",
    "        if sib.name == \"p\":\n",
    "            prev_p = sib\n",
    "            break\n",
    "\n",
    "    # prev_p があればそこまで削除\n",
    "    keep_node = prev_p if prev_p else target_p\n",
    "    found_flag = False\n",
    "    for child in list(body.children):\n",
    "        if child is keep_node:\n",
    "            found_flag = True\n",
    "        if not found_flag:\n",
    "            child.decompose()\n",
    "\n",
    "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "    print(f\"[OK] '{keyword}' より前の不要要素を削除 => {html_file}\")\n",
    "\n",
    "def main():\n",
    "    index_url = \"https://la-concur-standard-support.github.io/concur-standard-docs/index.htm\"\n",
    "    print(f\"Indexページ: {index_url} を解析します...\")\n",
    "    index_list = parse_index_page(index_url)\n",
    "    if not index_list:\n",
    "        print(\"index_listが空。終了\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== ドキュメント一覧 ===\")\n",
    "    for i,info in enumerate(index_list, start=1):\n",
    "        doc_url = info[\"DocDownloadURL\"] or \"None\"\n",
    "        print(f\"{i}. {info['GuideNameEn']} / {info['GuideNameJp']} => {doc_url}\")\n",
    "\n",
    "    sel = input(\"\\nどのDOCファイルを変換しますか？(番号入力, Enterで終了): \")\n",
    "    if not sel.strip():\n",
    "        print(\"キャンセル終了\")\n",
    "        return\n",
    "    idx = int(sel)\n",
    "    if idx<1 or idx>len(index_list):\n",
    "        print(\"番号不正\")\n",
    "        return\n",
    "\n",
    "    chosen = index_list[idx-1]\n",
    "    doc_download_url = chosen[\"DocDownloadURL\"]\n",
    "    if not doc_download_url:\n",
    "        print(\"DOCファイルURLなし。変換不可\")\n",
    "        return\n",
    "\n",
    "    # ダウンロード\n",
    "    print(\"ダウンロードURL:\", doc_download_url)\n",
    "    r = requests.get(doc_download_url)\n",
    "    if r.status_code!=200:\n",
    "        print(\"DL失敗 HTTP\", r.status_code)\n",
    "        return\n",
    "    fname = os.path.basename(urlsplit(doc_download_url).path)\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"保存:\", fname)\n",
    "\n",
    "    # .doc → .docx (LibreOffice)\n",
    "    if not fname.lower().endswith(\".docx\"):\n",
    "        try:\n",
    "            docx_file = doc_to_docx_via_libreoffice(fname)\n",
    "        except Exception as e:\n",
    "            print(\"doc->docx変換失敗:\", e)\n",
    "            return\n",
    "    else:\n",
    "        docx_file = fname\n",
    "\n",
    "    # docx → html (Mammoth)\n",
    "    html_file = os.path.splitext(docx_file)[0] + \".html\"\n",
    "    mammoth_docx_to_html(docx_file, html_file)\n",
    "\n",
    "    # 1) 表紙枠削除\n",
    "    remove_top_frame(html_file)\n",
    "\n",
    "    # 2) テーブル強制罫線\n",
    "    force_inline_table_border(html_file)\n",
    "\n",
    "    # 3) 「設定ガイド」より前を削除（直前の<p>は残す）\n",
    "    remove_top_until_keyword_keep_previous(html_file, \"設定ガイド\")\n",
    "\n",
    "    print(\"最終HTML:\", html_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeezfgkXDzh7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQOHx6J_F4XZ"
   },
   "source": [
    "# HTML Generaror 2: 手動で修正したWORD版の開発ガイドをHTML版に移植"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18847,
     "status": "ok",
     "timestamp": 1750921193649,
     "user": {
      "displayName": "AI Project Hub",
      "userId": "11185609607917065172"
     },
     "user_tz": -540
    },
    "id": "uW4O1X5weuR5",
    "outputId": "6d71c0fb-43ed-4504-d258-105c16b6c52f"
   },
   "outputs": [],
   "source": [
    "# Colabセルで実行\n",
    "!apt-get update -qq\n",
    "!apt-get install -y libreoffice\n",
    "!pip install mammoth beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1204948,
     "status": "error",
     "timestamp": 1750922398609,
     "user": {
      "displayName": "AI Project Hub",
      "userId": "11185609607917065172"
     },
     "user_tz": -540
    },
    "id": "ijXm0CWb-UX7",
    "outputId": "be8d5896-98a5-4b85-eb4a-ebdce7655684"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import mammoth\n",
    "from bs4 import BeautifulSoup\n",
    "from google.colab import files\n",
    "\n",
    "# =============== 各種関数 ===============\n",
    "\n",
    "def doc_to_docx_via_libreoffice(doc_path):\n",
    "    \"\"\"LibreOffice (soffice)で .doc → .docx に変換する\"\"\"\n",
    "    soffice_path = \"/usr/bin/soffice\"\n",
    "    cmd = [\n",
    "        soffice_path, \"--headless\", \"--convert-to\", \"docx\",\n",
    "        doc_path, \"--outdir\",\n",
    "        os.path.dirname(doc_path) if os.path.dirname(doc_path) else \".\"\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "    base, ext = os.path.splitext(doc_path)\n",
    "    docx_file = base + \".docx\"\n",
    "    if not os.path.exists(docx_file):\n",
    "        raise FileNotFoundError(f\"{docx_file} が生成されませんでした。.doc→.docx失敗？\")\n",
    "    return docx_file\n",
    "\n",
    "def mammoth_docx_to_html(docx_file, html_file):\n",
    "    \"\"\"Mammoth で docx→HTML\"\"\"\n",
    "    with open(docx_file, \"rb\") as f:\n",
    "        result = mammoth.convert_to_html(f)\n",
    "    html_content = result.value\n",
    "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_content)\n",
    "    print(f\"[OK] {docx_file} → {html_file} (HTML化)\")\n",
    "\n",
    "def ensure_body(soup):\n",
    "    \"\"\"soupにbodyが無ければ作成してコンテンツを移動\"\"\"\n",
    "    if not soup.body:\n",
    "        new_body = soup.new_tag(\"body\")\n",
    "        # head要素以外の子要素を body に移動\n",
    "        for child in list(soup.contents):\n",
    "            if child.name != \"head\":\n",
    "                new_body.append(child.extract())\n",
    "        soup.append(new_body)\n",
    "    return soup\n",
    "\n",
    "def remove_top_frame(html_path):\n",
    "    \"\"\"HTML上部の表紙枠<div>や先頭のBase64画像などを削除\"\"\"\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    soup = ensure_body(soup)\n",
    "    body = soup.body\n",
    "\n",
    "    # (1) div style=\"border:...\" 削除\n",
    "    candidate_divs = body.find_all(\"div\", style=re.compile(r\"(?i)border\"))\n",
    "    if candidate_divs:\n",
    "        top_div = candidate_divs[0]\n",
    "        top_div.decompose()\n",
    "\n",
    "    # (2) 先頭の base64画像 <p><img...> 削除\n",
    "    paragraphs = body.find_all(\"p\")\n",
    "    for ptag in paragraphs:\n",
    "        img = ptag.find(\"img\", src=re.compile(r\"^data:image/png;base64\"))\n",
    "        if img:\n",
    "            ptag.decompose()\n",
    "            break\n",
    "\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "    print(f\"[OK] 表紙枠/画像を削除 => {html_path}\")\n",
    "\n",
    "def force_inline_table_border(html_path):\n",
    "    \"\"\"tableタグ等に強制的に枠線を付与\"\"\"\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    soup = ensure_body(soup)\n",
    "    body = soup.body\n",
    "\n",
    "    # (A) tableタグに border='1' を付与\n",
    "    tables = body.find_all(\"table\")\n",
    "    for tbl in tables:\n",
    "        tbl['border'] = \"1\"\n",
    "\n",
    "    # (B) table/tr/td/th の inline-style から 'border:none' を除去し、'border:1px solid black' を付与\n",
    "    for tag in body.find_all([\"table\",\"tr\",\"th\",\"td\"]):\n",
    "        old_style = tag.get(\"style\",\"\")\n",
    "        # 'border: none' を削除\n",
    "        new_style = re.sub(r\"border\\s*:\\s*none[^;]*;?\", \"\", old_style, flags=re.IGNORECASE)\n",
    "        if tag.name == \"table\":\n",
    "            # tableの場合\n",
    "            new_style += \";border:1px solid black;border-collapse:collapse;\"\n",
    "        else:\n",
    "            # tr, th, tdの場合\n",
    "            new_style += \";border:1px solid black;\"\n",
    "        tag[\"style\"] = new_style.strip(\"; \")\n",
    "\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "    print(f\"[OK] テーブル枠線をインラインで強制 =>\", html_path)\n",
    "\n",
    "def remove_top_until_keyword_keep_previous(html_file, keyword=\"設定ガイド\"):\n",
    "    \"\"\"指定キーワードを含む<p>の直前の<p>だけ残し、それより上を削除\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    soup = ensure_body(soup)\n",
    "    body = soup.body\n",
    "\n",
    "    target_p = body.find(\"p\", string=lambda txt: txt and keyword in txt)\n",
    "    if not target_p:\n",
    "        print(f\"'{keyword}' を含む <p> が見つからないためスキップします\")\n",
    "        with open(html_file, \"w\", encoding=\"utf-8\") as f2:\n",
    "            f2.write(str(soup))\n",
    "        return\n",
    "\n",
    "    prev_p = None\n",
    "    for sib in target_p.previous_siblings:\n",
    "        if sib.name == \"p\":\n",
    "            prev_p = sib\n",
    "            break\n",
    "\n",
    "    # prev_p があればそこまで削除\n",
    "    keep_node = prev_p if prev_p else target_p\n",
    "    found_flag = False\n",
    "    for child in list(body.children):\n",
    "        if child is keep_node:\n",
    "            found_flag = True\n",
    "        if not found_flag:\n",
    "            child.decompose()\n",
    "\n",
    "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "    print(f\"[OK] '{keyword}' より前の不要要素を削除 => {html_file}\")\n",
    "\n",
    "# =============== メイン処理 (Colab上で実行) ===============\n",
    "def convert_word_to_html_with_cleanup(keyword=\"設定ガイド\"):\n",
    "    \"\"\"\n",
    "    GoogleColab上で .doc/.docx をアップロード→HTML変換→後処理まで実行\n",
    "    \"\"\"\n",
    "    # ファイルアップロード\n",
    "    print(\"Word(.doc/.docx)ファイルをアップロードしてください...\")\n",
    "    uploaded = files.upload()\n",
    "    if not uploaded:\n",
    "        print(\"アップロードがキャンセルされました。終了します。\")\n",
    "        return\n",
    "\n",
    "    for filename in uploaded.keys():\n",
    "        print(f\"\\n--- 処理を開始します: {filename} ---\")\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        ext_lower = ext.lower()\n",
    "\n",
    "        # 1) .doc を .docx に変換（LibreOffice）\n",
    "        if ext_lower == \".doc\":\n",
    "            try:\n",
    "                docx_file = doc_to_docx_via_libreoffice(filename)\n",
    "            except Exception as e:\n",
    "                print(\"doc->docx変換失敗:\", e)\n",
    "                continue\n",
    "        elif ext_lower == \".docx\":\n",
    "            # docxならそのまま使う\n",
    "            docx_file = filename\n",
    "        else:\n",
    "            print(\"対象外の拡張子です。スキップします。\")\n",
    "            continue\n",
    "\n",
    "        # 2) docx→html (Mammoth)\n",
    "        html_file = base + \".html\"  # 例: sample.docx → sample.html\n",
    "        mammoth_docx_to_html(docx_file, html_file)\n",
    "\n",
    "        # 3) 不要部分の削除やテーブル罫線追加など (必要に応じて削除/改変OK)\n",
    "        remove_top_frame(html_file)  # 表紙枠と先頭画像を削除\n",
    "        force_inline_table_border(html_file)  # テーブルに枠線を付与\n",
    "        remove_top_until_keyword_keep_previous(html_file, keyword)  # キーワード(\"設定ガイド\")前を削除\n",
    "\n",
    "        # 4) 変換結果のダウンロード (Colab)\n",
    "        print(f\"\\n変換完了。ローカル保存: {html_file}\")\n",
    "        print(\"HTMLファイルをダウンロードします...\")\n",
    "        files.download(html_file)\n",
    "        print(\"\\n------------------------------------------\")\n",
    "\n",
    "# 実行\n",
    "convert_word_to_html_with_cleanup(keyword=\"設定ガイド\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
