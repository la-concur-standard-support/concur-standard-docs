{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3150,
     "status": "ok",
     "timestamp": 1751527718817,
     "user": {
      "displayName": "la-concur-standard-support",
      "userId": "04856838222038801438"
     },
     "user_tz": -540
    },
    "id": "dWSJoA3Zzrq0",
    "outputId": "65bc2765-840d-4212-80c9-05832471799a"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "def parse_index_page(index_url):\n",
    "    resp = requests.get(index_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    result_list = []\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        print(\"テーブルが見つかりません。HTML構造を確認してください。\")\n",
    "        return result_list\n",
    "\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        # 今回は4列 (英名, 日名, ダウンロード, URL) を想定\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        guide_en = cols[0].get_text(strip=True)\n",
    "        guide_jp = cols[1].get_text(strip=True)\n",
    "        # cols[2] は \"DOC - PDF\" など不要なら無視してOK\n",
    "        base_url = cols[3].get_text(strip=True)\n",
    "\n",
    "        result_list.append({\n",
    "            \"GuideNameEn\": guide_en,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"BaseURL\": base_url\n",
    "        })\n",
    "    return result_list\n",
    "\n",
    "def remove_newlines_and_dots(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\r\\n]+\", \"\", text)\n",
    "    text = re.sub(r\"[.…]+\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def parse_toc_from_url(url, doc_name, guide_en, guide_jp):\n",
    "    \"\"\"\n",
    "    doc_name, guide_en, guide_jpを使って、\n",
    "    url上で #__RefHeading__ や #_bookmark のアンカーを探し →\n",
    "    (SectionTitle1, SectionTitle2, AnchorID, PageNumberなど)を返す\n",
    "    \"\"\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"HTTPエラー: {resp.status_code}\")\n",
    "        return []\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # ＜修正箇所＞アンカーIDのパターンを拡張\n",
    "    # startswith(\"#__RefHeading__\") に加え、startswith(\"#_bookmark\") などを拾う\n",
    "    toc_links = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        # 例: #__RefHeading___TocXXXX, #_bookmark0, #_bookmark1 ...\n",
    "        if (href.startswith(\"#__RefHeading__\")\n",
    "            or href.startswith(\"#_bookmark\")\n",
    "            or href.startswith(\"#_Toc\")):\n",
    "            toc_links.append(a)\n",
    "\n",
    "    print(f\"URL={url} から {len(toc_links)} 個のTOCリンクを検出\")\n",
    "\n",
    "    records = []\n",
    "    lastSection1 = \"\"\n",
    "\n",
    "    for link in toc_links:\n",
    "        raw_href = link.get(\"href\",\"\")\n",
    "        # 例: #__RefHeading___Toc485730093 or #_bookmark0\n",
    "        # '#' を取り除いて実際の AnchorID を取り出す\n",
    "        anchor_id = raw_href.lstrip(\"#\")\n",
    "\n",
    "        raw_text = link.get_text()\n",
    "\n",
    "        # 末尾の数字をページ番号として取り出すかどうか\n",
    "        match_page = re.search(r\"(\\d+)$\", raw_text)\n",
    "        if match_page:\n",
    "            page_num = match_page.group(1)\n",
    "            raw_text = re.sub(r\"\\s*\\d+$\", \"\", raw_text)\n",
    "        else:\n",
    "            page_num = \"\"\n",
    "\n",
    "        text_val = remove_newlines_and_dots(raw_text)\n",
    "\n",
    "        # \"セクション X:\" の判定\n",
    "        match_sec = re.match(r\"^(セクション\\s*\\d+)\\s*:\\s*(.*)$\", text_val)\n",
    "        if match_sec:\n",
    "            section1 = remove_newlines_and_dots(match_sec.group(1))\n",
    "            section2 = remove_newlines_and_dots(match_sec.group(2))\n",
    "            lastSection1 = section1\n",
    "        else:\n",
    "            # 直近のセクション1を流用\n",
    "            section1 = lastSection1\n",
    "            section2 = text_val\n",
    "\n",
    "        # full_link\n",
    "        full_link = f\"{url}{raw_href}\"\n",
    "\n",
    "        rec = {\n",
    "            \"DocName\": doc_name,\n",
    "            \"GuideNameEn\": guide_en,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"SectionTitle1\": section1,\n",
    "            \"SectionTitle2\": section2,\n",
    "            \"PageNumber\": page_num,\n",
    "            \"AnchorID\": anchor_id,\n",
    "            \"BaseURL\": url,\n",
    "            \"FullLink\": full_link,\n",
    "            \"Remarks\": text_val\n",
    "        }\n",
    "        records.append(rec)\n",
    "    return records\n",
    "\n",
    "def update_mapping_db(mapping_csv_path, new_records, base_url):\n",
    "    if os.path.exists(mapping_csv_path):\n",
    "        df = pd.read_csv(mapping_csv_path, encoding='utf-8-sig')\n",
    "        print(f\"{mapping_csv_path} 読み込み: {len(df)} 行\")\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\n",
    "            \"DocName\",\"GuideNameEn\",\"GuideNameJp\",\"SectionTitle1\",\"SectionTitle2\",\n",
    "            \"PageNumber\",\"AnchorID\",\"BaseURL\",\"FullLink\",\"Remarks\"\n",
    "        ])\n",
    "        print(f\"{mapping_csv_path} が無いので新規作成\")\n",
    "\n",
    "    before_count = len(df)\n",
    "    # 同じBaseURLの行を削除 (既存データをアップデートするイメージ)\n",
    "    df = df[df[\"BaseURL\"] != base_url]\n",
    "    after_count = len(df)\n",
    "    print(f\"BaseURL='{base_url}' の行を {before_count - after_count}件 削除\")\n",
    "\n",
    "    new_df = pd.DataFrame(new_records)\n",
    "    # 結合\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "    df.to_csv(mapping_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"{mapping_csv_path} を更新 (最終行数={len(df)})\")\n",
    "\n",
    "def main():\n",
    "    mapping_csv = \"mapping_DB.csv\"\n",
    "    index_url = \"https://la-concur-standard-support.github.io/concur-standard-docs/index.htm\"\n",
    "\n",
    "    print(f\"Indexページ: {index_url} を解析中...\")\n",
    "    index_list = parse_index_page(index_url)\n",
    "    if not index_list:\n",
    "        print(\"index_listが空です。終了します。\")\n",
    "        return\n",
    "\n",
    "    # ユーザに どのdocを処理するか 選択してもらう\n",
    "    print(\"\\n=== 現在インデックスにあるドキュメント一覧 ===\")\n",
    "    for i, info in enumerate(index_list, start=1):\n",
    "        print(f\"{i}. {info['GuideNameEn']} / {info['GuideNameJp']} => {info['BaseURL']}\")\n",
    "\n",
    "    sel = input(\"\\n何番のドキュメントを処理しますか？番号を入力(Enterで終了): \")\n",
    "    if not sel.strip():\n",
    "        print(\"キャンセル。終了。\")\n",
    "        return\n",
    "    sel_idx = int(sel)\n",
    "\n",
    "    if sel_idx < 1 or sel_idx > len(index_list):\n",
    "        print(\"不正な番号です。終了。\")\n",
    "        return\n",
    "\n",
    "    chosen = index_list[sel_idx - 1]\n",
    "    guide_en = chosen[\"GuideNameEn\"]\n",
    "    guide_jp = chosen[\"GuideNameJp\"]\n",
    "    base_url = chosen[\"BaseURL\"]\n",
    "\n",
    "    # ＜修正箇所＞ doc_name を .html/.htm どちらにも対応させて .docx に置換\n",
    "    fname = os.path.basename(urlsplit(base_url).path)  # e.g. \"Exp_SG_Account_Codes-jp.html\"\n",
    "    # \".html\" も \".htm\" もまとめて \".docx\" にする\n",
    "    doc_name = re.sub(r\"\\.html?$\", \".docx\", fname, flags=re.IGNORECASE)\n",
    "\n",
    "    print(f\"\\n--- 選択ドキュメント ---\\nDocName: {doc_name}\\nGuideNameEn: {guide_en}\\nGuideNameJp: {guide_jp}\\nBaseURL: {base_url}\\n\")\n",
    "\n",
    "    # 目次解析\n",
    "    records = parse_toc_from_url(base_url, doc_name, guide_en, guide_jp)\n",
    "    if not records:\n",
    "        print(\"目次が空です。終了。\")\n",
    "        return\n",
    "\n",
    "    # DB更新\n",
    "    update_mapping_db(mapping_csv, records, base_url)\n",
    "    print(\"処理が完了しました。\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li0FY1Zrjk0Z"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_h93UaB9Bk8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
