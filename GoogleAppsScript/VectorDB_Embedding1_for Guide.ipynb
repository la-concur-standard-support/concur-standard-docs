{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnvUqiSEZFkS"
   },
   "source": [
    "# VectorDB_Embedding１：ダイレクトにIndexから文書を選びベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28741,
     "status": "ok",
     "timestamp": 1751528019643,
     "user": {
      "displayName": "la-concur-standard-support",
      "userId": "04856838222038801438"
     },
     "user_tz": -540
    },
    "id": "9Dcrstdii4TM",
    "outputId": "659867d5-4498-46a1-8319-269cc0c96ed0"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y openai\n",
    "!pip install openai==0.28.0\n",
    "\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv\n",
    "!pip install pinecone==6.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12277,
     "status": "ok",
     "timestamp": 1751528362708,
     "user": {
      "displayName": "la-concur-standard-support",
      "userId": "04856838222038801438"
     },
     "user_tz": -540
    },
    "id": "Og1EzEUGp4I1",
    "outputId": "307d0350-c84a-4c37-f290-0d418a77b40e"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time  # ★ sleep用\n",
    "import pandas as pd\n",
    "from urllib.parse import urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from pinecone import NotFoundException  # ← 例外ハンドリング用\n",
    "\n",
    "############################\n",
    "# 1) APIキー読み込み\n",
    "############################\n",
    "def load_api_keys(txt_filepath=\"api_keys_standard.txt\"):\n",
    "    if not os.path.exists(txt_filepath):\n",
    "        raise FileNotFoundError(f\"APIキーのファイルが見つかりません: {txt_filepath}\")\n",
    "\n",
    "    # BOM 付き UTF-8 でも読めるように encoding=\"utf-8\"\n",
    "    with open(txt_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" in line:\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            os.environ[k.strip()] = v.strip()\n",
    "\n",
    "load_api_keys(\"api_keys_standard.txt\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "OPENAI_API_KEY   = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "openai.api_key   = OPENAI_API_KEY\n",
    "\n",
    "INDEX_NAME = \"concur-index-full\"\n",
    "NAMESPACE  = \"demo-html\"\n",
    "\n",
    "############################\n",
    "# 2) Pinecone 初期化\n",
    "############################\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# ▼▼ ここで全部消していた処理をコメントアウト ▼▼\n",
    "# try:\n",
    "#     index.delete(deleteAll=True, namespace=NAMESPACE)\n",
    "#     print(f\"[INFO] Deleted all vectors in namespace '{NAMESPACE}'.\")\n",
    "# except NotFoundException:\n",
    "#     print(f\"[WARN] '{NAMESPACE}' namespace not found. (No problem, skipping.)\")\n",
    "\n",
    "stats = index.describe_index_stats()\n",
    "ns_info = stats.get(\"namespaces\", {})\n",
    "if NAMESPACE in ns_info:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' 既に存在: vector_count={ns_info[NAMESPACE]['vector_count']}\")\n",
    "else:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' はまだ存在しません。\")\n",
    "\n",
    "\n",
    "############################\n",
    "# 3) indexページをパース (HTMLスクレイプ)\n",
    "############################\n",
    "def parse_index_page(index_url):\n",
    "    resp = requests.get(index_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    result_list = []\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        print(\"[WARN] テーブルが見つかりません:\", index_url)\n",
    "        return result_list\n",
    "\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        guide_en = cols[0].get_text(strip=True)\n",
    "        guide_jp = cols[1].get_text(strip=True)\n",
    "        base_url = cols[3].get_text(strip=True)\n",
    "        result_list.append({\n",
    "            \"GuideNameEn\": guide_en,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"BaseURL\": base_url\n",
    "        })\n",
    "    return result_list\n",
    "\n",
    "\n",
    "############################\n",
    "# 4) mapping_DB.csv ロード\n",
    "############################\n",
    "def load_mapping_db(csv_path=\"mapping_DB.csv\"):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"[ERROR] mapping_DB.csv が見つかりません: {csv_path}\")\n",
    "\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "############################\n",
    "# 5) HTMLソース取得\n",
    "############################\n",
    "def fetch_html(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "############################\n",
    "# 6) アンカーoffset検索\n",
    "############################\n",
    "def find_anchor_offset(html_src, anchor_id):\n",
    "    pattern = re.compile(\n",
    "        rf'<a[^>]+(?:id|name)\\s*=\\s*[\"\\']{re.escape(anchor_id)}[\"\\']',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m = pattern.search(html_src)\n",
    "    if m:\n",
    "        return m.start()\n",
    "    return -1\n",
    "\n",
    "############################\n",
    "# 7) chunking & embedding\n",
    "############################\n",
    "def get_embedding(text: str, model=\"text-embedding-ada-002\"):\n",
    "    resp = openai.Embedding.create(model=model, input=text)\n",
    "    return resp[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def chunk_text(text: str, chunk_size=2000):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "    while start < length:\n",
    "        end = min(start + chunk_size, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "\n",
    "############################\n",
    "# メインフロー\n",
    "############################\n",
    "def main():\n",
    "    # 1) indexページをスクレイプしてガイド一覧を取得\n",
    "    index_url = \"https://la-concur-standard-support.github.io/concur-standard-docs/index.htm\"\n",
    "    docs = parse_index_page(index_url)\n",
    "    if not docs:\n",
    "        print(\"[ERROR] indexページからガイド情報が取得できませんでした。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== ガイド一覧 ===\")\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        print(f\"{i}. {d['GuideNameEn']} / {d['GuideNameJp']} => {d['BaseURL']}\")\n",
    "\n",
    "    sel = input(\"\\nどのガイドを処理しますか？(番号): \").strip()\n",
    "    if not sel.isdigit():\n",
    "        print(\"キャンセル\")\n",
    "        return\n",
    "    idx = int(sel)\n",
    "    if idx < 1 or idx > len(docs):\n",
    "        print(\"[ERROR] 選択が範囲外。終了。\")\n",
    "        return\n",
    "\n",
    "    chosen = docs[idx-1]\n",
    "    base_url = chosen[\"BaseURL\"]\n",
    "    guide_en = chosen[\"GuideNameEn\"]\n",
    "    guide_jp = chosen[\"GuideNameJp\"]\n",
    "\n",
    "    # e.g. \"Exp_SG_Account_Codes-jp.html\" -> \"Exp_SG_Account_Codes-jp.docx\"\n",
    "    fname = os.path.basename(urlsplit(base_url).path)\n",
    "    doc_name = re.sub(r\"\\.html?$\", \".docx\", fname, flags=re.IGNORECASE)\n",
    "\n",
    "    print(f\"\\n選択されたガイド => doc_name={doc_name}, base_url={base_url}\")\n",
    "\n",
    "    # 2) mapping_DB.csv をロード\n",
    "    mapping_rows = load_mapping_db(\"mapping_DB.csv\")\n",
    "    if not mapping_rows:\n",
    "        print(\"[WARN] mapping_DB.csv から行が読み込めませんでした。\")\n",
    "        return\n",
    "\n",
    "    # doc_name 一致行を抽出\n",
    "    doc_rows = [r for r in mapping_rows if r.get(\"DocName\",\"\") == doc_name]\n",
    "    if not doc_rows:\n",
    "        print(f\"[WARN] DocName='{doc_name}' の行がmapping_DB.csvにありません。終了。\")\n",
    "        return\n",
    "\n",
    "    # 3) HTMLを取得\n",
    "    html_src = fetch_html(base_url)\n",
    "    print(f\"[INFO] HTMLソース取得: 長さ={len(html_src)}\")\n",
    "\n",
    "    # 4) doc_rows を PageNumber + AnchorID でソート\n",
    "    def sort_key(row):\n",
    "        p = 999999\n",
    "        if row[\"PageNumber\"].isdigit():\n",
    "            p = int(row[\"PageNumber\"])\n",
    "        return (p, row[\"AnchorID\"])\n",
    "    sorted_rows = sorted(doc_rows, key=sort_key)\n",
    "\n",
    "    # 5) offset計算\n",
    "    offsets = []\n",
    "    for row in sorted_rows:\n",
    "        anchor_id = row[\"AnchorID\"]\n",
    "        off = find_anchor_offset(html_src, anchor_id)\n",
    "        offsets.append({ \"row\": row, \"offset\": off })\n",
    "\n",
    "    ############################\n",
    "    # ▼▼ バッチアップサート ▼▼\n",
    "    ############################\n",
    "\n",
    "    BATCH_SIZE = 100\n",
    "    vectors_buffer = []\n",
    "    doc_id_counter = 0\n",
    "\n",
    "    def flush_upsert_buffer():\n",
    "        nonlocal vectors_buffer\n",
    "        if not vectors_buffer:\n",
    "            return\n",
    "        print(f\"[BATCH UPSERT] {len(vectors_buffer)} vectors ...\")\n",
    "        resp = index.upsert(vectors=vectors_buffer, namespace=NAMESPACE)\n",
    "        print(\"[BATCH RESP]:\", resp)\n",
    "        vectors_buffer = []\n",
    "        time.sleep(1)\n",
    "\n",
    "    # offset[i]~offset[i+1] で区間抽出 => chunk化 => embedding => upsert\n",
    "    for i in range(len(offsets)):\n",
    "        cur = offsets[i]\n",
    "        row  = cur[\"row\"]\n",
    "        off_i = cur[\"offset\"]\n",
    "        if off_i < 0:\n",
    "            off_i = 0\n",
    "\n",
    "        if i < len(offsets)-1:\n",
    "            off_j = offsets[i+1][\"offset\"]\n",
    "            if off_j < 0:\n",
    "                off_j = len(html_src)\n",
    "        else:\n",
    "            off_j = len(html_src)\n",
    "\n",
    "        if off_j < off_i:\n",
    "            off_j = off_i\n",
    "\n",
    "        raw_segment = html_src[off_i:off_j]\n",
    "        text_str = re.sub(r\"<[^>]+>\", \"\", raw_segment)\n",
    "        text_str = re.sub(r\"\\s+\", \" \", text_str).strip()\n",
    "        if not text_str:\n",
    "            continue\n",
    "\n",
    "        doc_name_val = row[\"DocName\"]\n",
    "        guide_jp_val = row[\"GuideNameJp\"]\n",
    "        sec1         = row[\"SectionTitle1\"]\n",
    "        sec2         = row[\"SectionTitle2\"]\n",
    "        anchor_val   = row[\"AnchorID\"]\n",
    "        link_val     = row[\"FullLink\"]\n",
    "\n",
    "        # セクションタイトルと本文の間に改行\n",
    "        combined_text = f\"{sec1}\\n{sec2}\\n\\n{text_str}\"\n",
    "\n",
    "        chunk_list = chunk_text(combined_text, chunk_size=2000)\n",
    "\n",
    "        for chunk_str_data in chunk_list:\n",
    "            doc_id_counter += 1\n",
    "            chunk_id = f\"{doc_name_val}_chunk{doc_id_counter}\"\n",
    "\n",
    "            emb = get_embedding(chunk_str_data)\n",
    "            meta = {\n",
    "                \"DocName\": doc_name_val,\n",
    "                \"GuideNameJp\": guide_jp_val,\n",
    "                \"SectionTitle1\": sec1,\n",
    "                \"SectionTitle2\": sec2,\n",
    "                \"AnchorID\": anchor_val,\n",
    "                \"FullLink\": link_val,\n",
    "                \"chunk_text\": chunk_str_data\n",
    "            }\n",
    "\n",
    "            vectors_buffer.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"values\": emb,\n",
    "                \"metadata\": meta\n",
    "            })\n",
    "\n",
    "            if len(vectors_buffer) >= BATCH_SIZE:\n",
    "                flush_upsert_buffer()\n",
    "\n",
    "    # バッファに残ったデータを最後にアップサート\n",
    "    flush_upsert_buffer()\n",
    "    print(\"[INFO] 全アップサート完了\")\n",
    "\n",
    "    # テスト検索\n",
    "    test_query = \"ConcurのAPIでレシート登録したい\"\n",
    "    print(f\"\\n[INFO] テスト検索: {test_query}\")\n",
    "    q_emb = get_embedding(test_query)\n",
    "    sr = index.query(\n",
    "        vector=q_emb,\n",
    "        top_k=3,\n",
    "        include_metadata=True,\n",
    "        namespace=NAMESPACE\n",
    "    )\n",
    "    for match in sr.matches:\n",
    "        md = match.metadata\n",
    "        print(f\"- ID={match.id}, Score={match.score}\")\n",
    "        print(\"  SectionTitle2:\", md.get(\"SectionTitle2\"))\n",
    "        print(\"  FullLink:\", md.get(\"FullLink\"))\n",
    "        print(\"  chunk_text:\", md.get(\"chunk_text\",\"\")[:80], \"...\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvCSzhxIY_by"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9xAcfF8Y8b6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMuRVvjPZbPq"
   },
   "source": [
    "## VectorDB_Embedding2：手元のWORD文書からベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "boXKeWJ8Y9Co",
    "outputId": "40f68431-d094-4d58-e49d-97c08b1cc7e9"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y openai\n",
    "!pip install openai==0.28.0\n",
    "\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv\n",
    "!pip install pinecone==6.0.1\n",
    "!pip install python-docx\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import docx\n",
    "import pandas as pd\n",
    "import time  # ← バッチアップサート時に少し待機するため\n",
    "from urllib.parse import urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "############################\n",
    "# 1) APIキー読み込み\n",
    "############################\n",
    "def load_api_keys(txt_filepath=\"api_keys_standard.txt\"):\n",
    "    if not os.path.exists(txt_filepath):\n",
    "        raise FileNotFoundError(f\"APIキーのファイルが見つかりません: {txt_filepath}\")\n",
    "\n",
    "    with open(txt_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" in line:\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            os.environ[k.strip()] = v.strip()\n",
    "\n",
    "load_api_keys(\"api_keys_standard.txt\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "OPENAI_API_KEY   = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "openai.api_key   = OPENAI_API_KEY\n",
    "\n",
    "INDEX_NAME = \"concur-index-full\"\n",
    "NAMESPACE  = \"demo-html\"\n",
    "\n",
    "############################\n",
    "# 2) Pinecone 初期化\n",
    "############################\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "stats = index.describe_index_stats()\n",
    "ns_info = stats.get(\"namespaces\", {})\n",
    "if NAMESPACE in ns_info:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' 既に存在: vector_count={ns_info[NAMESPACE]['vector_count']}\")\n",
    "else:\n",
    "    print(f\"[INFO] namespace '{NAMESPACE}' はまだ存在しません。\")\n",
    "\n",
    "\n",
    "############################\n",
    "# 3) indexページ (HTML) をパース\n",
    "############################\n",
    "def parse_index_page(index_url):\n",
    "    resp = requests.get(index_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    result_list = []\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        print(\"[WARN] テーブルが見つかりません:\", index_url)\n",
    "        return result_list\n",
    "\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        guide_en = cols[0].get_text(strip=True)\n",
    "        guide_jp = cols[1].get_text(strip=True)\n",
    "        base_url = cols[3].get_text(strip=True)\n",
    "        result_list.append({\n",
    "            \"GuideNameEn\": guide_en,\n",
    "            \"GuideNameJp\": guide_jp,\n",
    "            \"BaseURL\": base_url\n",
    "        })\n",
    "    return result_list\n",
    "\n",
    "############################\n",
    "# 4) mapping_DB.csv ロード\n",
    "############################\n",
    "def load_mapping_db(csv_path=\"mapping_DB.csv\"):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"[ERROR] mapping_DB.csv が見つかりません: {csv_path}\")\n",
    "\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "############################\n",
    "# 5) HTMLソース取得\n",
    "############################\n",
    "def fetch_html(url):\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "############################\n",
    "# 6) アンカーoffset検索\n",
    "############################\n",
    "def find_anchor_offset(html_src, anchor_id):\n",
    "    pattern = re.compile(\n",
    "        rf'<a[^>]+(?:id|name)\\s*=\\s*[\"\\']{re.escape(anchor_id)}[\"\\']',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m = pattern.search(html_src)\n",
    "    if m:\n",
    "        return m.start()\n",
    "    return -1\n",
    "\n",
    "############################\n",
    "# 7) chunking & embedding\n",
    "############################\n",
    "def get_embedding(text: str, model=\"text-embedding-ada-002\"):\n",
    "    resp = openai.Embedding.create(model=model, input=text)\n",
    "    return resp[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def chunk_text(text: str, chunk_size=2000):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "    while start < length:\n",
    "        end = min(start + chunk_size, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "############################\n",
    "# 8) Word文書(ローカル) からテキスト抽出\n",
    "############################\n",
    "def parse_local_word(docx_path):\n",
    "    if not os.path.exists(docx_path):\n",
    "        raise FileNotFoundError(f\"[ERROR] Word文書が見つかりません: {docx_path}\")\n",
    "\n",
    "    doc = docx.Document(docx_path)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    full_text = \"\\n\".join(paragraphs)\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) indexページからdocリストを取得\n",
    "    index_url = \"https://la-concur-standard-support.github.io/concur-standard-docs/index.htm\"\n",
    "    docs = parse_index_page(index_url)\n",
    "    if not docs:\n",
    "        print(\"[ERROR] indexページが見つかりません。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== ガイド一覧 ===\")\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        print(f\"{i}. {d['GuideNameEn']} / {d['GuideNameJp']} => {d['BaseURL']}\")\n",
    "\n",
    "    sel = input(\"\\nどのガイドを処理しますか？(番号): \").strip()\n",
    "    if not sel.isdigit():\n",
    "        print(\"[ERROR] キャンセル or 無効入力\")\n",
    "        return\n",
    "    idx = int(sel)\n",
    "    if idx < 1 or idx > len(docs):\n",
    "        print(\"[ERROR] 範囲外\")\n",
    "        return\n",
    "\n",
    "    chosen = docs[idx-1]\n",
    "    base_url = chosen[\"BaseURL\"]\n",
    "    guide_en = chosen[\"GuideNameEn\"]\n",
    "    guide_jp = chosen[\"GuideNameJp\"]\n",
    "\n",
    "    # 例: \"Exp_SG_Account_Codes-jp.html\" => \"Exp_SG_Account_Codes-jp.docx\"\n",
    "    fname = os.path.basename(urlsplit(base_url).path)\n",
    "    doc_name = re.sub(r\"\\.html?$\", \".docx\", fname, flags=re.IGNORECASE)\n",
    "\n",
    "    print(f\"\\n選択ガイド => doc_name={doc_name}, base_url={base_url}\")\n",
    "\n",
    "    # 2) mapping_DB.csv ロード\n",
    "    mapping_rows = load_mapping_db(\"mapping_DB.csv\")\n",
    "    if not mapping_rows:\n",
    "        print(\"[ERROR] mapping_DB.csv 読み込み失敗\")\n",
    "        return\n",
    "\n",
    "    # doc_name に一致する行\n",
    "    doc_rows = [r for r in mapping_rows if r.get(\"DocName\", \"\") == doc_name]\n",
    "\n",
    "    ############################\n",
    "    # ▼▼ バッチアップサート ▼▼\n",
    "    ############################\n",
    "\n",
    "    # Pineconeアップサート用の設定\n",
    "    BATCH_SIZE = 100\n",
    "    vectors_buffer = []\n",
    "    doc_id_counter = 0\n",
    "\n",
    "    def flush_upsert_buffer():\n",
    "        \"\"\"バッファの内容をまとめてUpsertし、バッファをクリア\"\"\"\n",
    "        nonlocal vectors_buffer\n",
    "        if not vectors_buffer:\n",
    "            return\n",
    "        print(f\"[BATCH UPSERT] {len(vectors_buffer)} vectors ...\")\n",
    "        resp = index.upsert(vectors=vectors_buffer, namespace=NAMESPACE)\n",
    "        print(\"[BATCH RESP]:\", resp)\n",
    "        # バッファをクリア\n",
    "        vectors_buffer = []\n",
    "        # 連打を避けるため少し待機\n",
    "        time.sleep(1)\n",
    "\n",
    "    # ================================\n",
    "    # (A) HTMLダウンロードの試行\n",
    "    # ================================\n",
    "    can_download = True\n",
    "    html_src = \"\"\n",
    "    try:\n",
    "        print(f\"[INFO] ダウンロード試行: {base_url}\")\n",
    "        html_src = fetch_html(base_url)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] ダウンロード失敗: {e}\")\n",
    "        can_download = False\n",
    "\n",
    "    if can_download and doc_rows:\n",
    "        # --- HTMLが成功ダウンロード → mapping_DBに従い、アンカー単位で分割 ---\n",
    "        print(\"[INFO] Webダウンロード成功 → mapping_DBに従い、アンカー単位で分割します。\")\n",
    "\n",
    "        # PageNumber + AnchorID でソート\n",
    "        def sort_key(r):\n",
    "            p = 999999\n",
    "            if r[\"PageNumber\"].isdigit():\n",
    "                p = int(r[\"PageNumber\"])\n",
    "            return (p, r[\"AnchorID\"])\n",
    "        sorted_rows = sorted(doc_rows, key=sort_key)\n",
    "\n",
    "        offsets = []\n",
    "        for row in sorted_rows:\n",
    "            anchor_id = row[\"AnchorID\"]\n",
    "            off = find_anchor_offset(html_src, anchor_id)\n",
    "            offsets.append({\"row\": row, \"offset\": off})\n",
    "\n",
    "        for i in range(len(offsets)):\n",
    "            cur = offsets[i]\n",
    "            r   = cur[\"row\"]\n",
    "            off_i = cur[\"offset\"]\n",
    "            if off_i < 0:\n",
    "                off_i = 0\n",
    "\n",
    "            if i < len(offsets)-1:\n",
    "                off_j = offsets[i+1][\"offset\"]\n",
    "                if off_j < 0:\n",
    "                    off_j = len(html_src)\n",
    "            else:\n",
    "                off_j = len(html_src)\n",
    "            if off_j < off_i:\n",
    "                off_j = off_i\n",
    "\n",
    "            raw_segment = html_src[off_i:off_j]\n",
    "            text_str = re.sub(r\"<[^>]+>\", \"\", raw_segment)\n",
    "            text_str = re.sub(r\"\\s+\", \" \", text_str).strip()\n",
    "            if not text_str:\n",
    "                continue\n",
    "\n",
    "            chunk_list = chunk_text(text_str, chunk_size=2000)\n",
    "\n",
    "            doc_name_val = r[\"DocName\"]\n",
    "            guide_jp_val = r[\"GuideNameJp\"]\n",
    "            sec1         = r[\"SectionTitle1\"]\n",
    "            sec2         = r[\"SectionTitle2\"]\n",
    "            anchor_val   = r[\"AnchorID\"]\n",
    "            link_val     = r[\"FullLink\"]\n",
    "\n",
    "            for chunk_str_data in chunk_list:\n",
    "                doc_id_counter += 1\n",
    "                chunk_id = f\"{doc_name_val}_chunk{doc_id_counter}\"\n",
    "\n",
    "                emb = get_embedding(chunk_str_data)\n",
    "                meta = {\n",
    "                    \"DocName\": doc_name_val,\n",
    "                    \"GuideNameJp\": guide_jp_val,\n",
    "                    \"SectionTitle1\": sec1,\n",
    "                    \"SectionTitle2\": sec2,\n",
    "                    \"AnchorID\": anchor_val,\n",
    "                    \"FullLink\": link_val,\n",
    "                    \"chunk_text\": chunk_str_data\n",
    "                }\n",
    "\n",
    "                vectors_buffer.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"values\": emb,\n",
    "                    \"metadata\": meta\n",
    "                })\n",
    "\n",
    "                # バッファが一定サイズに達したらUpsert\n",
    "                if len(vectors_buffer) >= BATCH_SIZE:\n",
    "                    flush_upsert_buffer()\n",
    "\n",
    "        # ループ完了後、残りがあればフラッシュ\n",
    "        flush_upsert_buffer()\n",
    "        print(\"[INFO] HTML文書のアップサート完了.\")\n",
    "\n",
    "    else:\n",
    "        # ================================\n",
    "        # (B) Word文書がローカルにあるパターン\n",
    "        # ================================\n",
    "        print(\"\\n[INFO] Word文書のローカルアップロード対応に切り替えます。\")\n",
    "        local_path = input(\"Word文書 (.docx) のローカルファイルパスを入力してください: \").strip()\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"[ERROR] 指定ファイルが見つかりません: {local_path}\")\n",
    "            return\n",
    "\n",
    "        # parse_local_word でテキスト全抽出\n",
    "        full_text = parse_local_word(local_path)\n",
    "        print(f\"[INFO] Word文書の段落合計文字数={len(full_text)}\")\n",
    "\n",
    "        # chunk化 & Embedding\n",
    "        chunked_data = chunk_text(full_text, chunk_size=2000)\n",
    "\n",
    "        doc_name_val = doc_name\n",
    "        guide_jp_val = guide_jp\n",
    "\n",
    "        for chunk_str_data in chunked_data:\n",
    "            doc_id_counter += 1\n",
    "            chunk_id = f\"{doc_name_val}_chunk{doc_id_counter}\"\n",
    "\n",
    "            emb = get_embedding(chunk_str_data)\n",
    "            meta = {\n",
    "                \"DocName\": doc_name_val,\n",
    "                \"GuideNameJp\": guide_jp_val,\n",
    "                \"FullLink\": \"(Local file, no link)\",\n",
    "                \"chunk_text\": chunk_str_data\n",
    "            }\n",
    "            vectors_buffer.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"values\": emb,\n",
    "                \"metadata\": meta\n",
    "            })\n",
    "\n",
    "            # バッファが一定サイズに達したらUpsert\n",
    "            if len(vectors_buffer) >= BATCH_SIZE:\n",
    "                flush_upsert_buffer()\n",
    "\n",
    "        # ループ完了後、残りがあればフラッシュ\n",
    "        flush_upsert_buffer()\n",
    "        print(\"[INFO] ローカルWord文書のアップサート完了.\")\n",
    "\n",
    "    # テスト検索\n",
    "    test_query = \"ConcurのAPIでレシート登録したい\"\n",
    "    print(f\"\\n[INFO] テスト検索: {test_query}\")\n",
    "    q_emb = get_embedding(test_query)\n",
    "    sr = index.query(\n",
    "        vector=q_emb,\n",
    "        top_k=3,\n",
    "        include_metadata=True,\n",
    "        namespace=NAMESPACE\n",
    "    )\n",
    "    for match in sr.matches:\n",
    "        md = match.metadata\n",
    "        print(f\"- ID={match.id}, Score={match.score}\")\n",
    "        print(\"  DocName:\", md.get(\"DocName\"))\n",
    "        print(\"  chunk_text:\", md.get(\"chunk_text\",\"\")[:80], \"...\")\n",
    "        print(\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
